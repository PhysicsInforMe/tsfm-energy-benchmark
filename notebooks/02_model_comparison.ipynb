{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Load Forecasting — Model Comparison\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PhysicsInforMe/scientific-prototypes/blob/main/energy-load-forecasting-benchmark/notebooks/02_model_comparison.ipynb)\n",
    "\n",
    "This notebook runs the full benchmark comparing **Time Series Foundation Models** (Chronos-Bolt, Chronos-2) against statistical baselines (Seasonal Naive, SARIMA) on ERCOT hourly load data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup (Colab)\n",
    "\n",
    "Uncomment and run the cell below when using Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Colab setup ---\n",
    "# !git clone https://github.com/PhysicsInforMe/scientific-prototypes.git\n",
    "# %cd scientific-prototypes/energy-load-forecasting-benchmark\n",
    "# !pip install -q -e \".[models]\"\n",
    "#\n",
    "# # Verify GPU\n",
    "# import torch\n",
    "# print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load ERCOT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "from energy_benchmark.data import ERCOTLoader\n",
    "from energy_benchmark.data.preprocessing import preprocess_series\n",
    "\n",
    "loader = ERCOTLoader(years=[2020, 2021, 2022, 2023, 2024])\n",
    "series = loader.load()\n",
    "series = preprocess_series(series)\n",
    "\n",
    "train, val, test = loader.split(series)\n",
    "print(f\"Train: {len(train):,} hrs | Val: {len(val):,} hrs | Test: {len(test):,} hrs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "train.plot(ax=ax, label=\"Train\", alpha=0.7)\n",
    "val.plot(ax=ax, label=\"Validation\", alpha=0.7)\n",
    "test.plot(ax=ax, label=\"Test\", alpha=0.7)\n",
    "ax.set_ylabel(\"Load (MW)\")\n",
    "ax.set_title(\"ERCOT Hourly Load — Train / Val / Test Split\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from energy_benchmark.models import SeasonalNaiveModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "models = []\n",
    "\n",
    "# --- Baselines ---\n",
    "models.append(SeasonalNaiveModel(seasonality=168))\n",
    "\n",
    "# --- Foundation Models ---\n",
    "try:\n",
    "    from energy_benchmark.models import ChronosBoltModel\n",
    "    models.append(ChronosBoltModel(model_size=\"base\", device=device))\n",
    "except ImportError:\n",
    "    print(\"chronos-forecasting not installed, skipping Chronos-Bolt\")\n",
    "\n",
    "try:\n",
    "    from energy_benchmark.models import Chronos2Model\n",
    "    models.append(Chronos2Model(device=device))\n",
    "except ImportError:\n",
    "    print(\"chronos-forecasting not installed, skipping Chronos-2\")\n",
    "\n",
    "print(f\"Models to evaluate: {[m.name for m in models]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit all models (loads weights for foundation models)\n",
    "for m in models:\n",
    "    print(f\"Fitting {m.name}...\")\n",
    "    m.fit(train)\n",
    "    print(f\"  {m.name} ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_benchmark.evaluation import BenchmarkRunner\n",
    "\n",
    "runner = BenchmarkRunner(\n",
    "    models=models,\n",
    "    prediction_horizons=[24, 168],\n",
    "    context_lengths=[512],\n",
    "    num_samples=100,\n",
    "    metric_names=[\"mae\", \"rmse\", \"mase\"],\n",
    ")\n",
    "\n",
    "results = runner.run(\n",
    "    train, test,\n",
    "    rolling_config={\"step_size\": 24, \"num_windows\": 30},\n",
    ")\n",
    "\n",
    "df = results.to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_benchmark.visualization import plot_comparison, plot_metric_heatmap\n",
    "\n",
    "for metric in [\"mae\", \"mase\"]:\n",
    "    if metric in df.columns:\n",
    "        plot_comparison(df, metric=metric)\n",
    "        plt.show()\n",
    "\n",
    "for metric in [\"mae\", \"mase\"]:\n",
    "    if metric in df.columns:\n",
    "        plot_metric_heatmap(df, metric=metric)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_benchmark.visualization import plot_probabilistic_forecast\n",
    "import numpy as np\n",
    "\n",
    "# Pick the first forecast window from the first model with samples\n",
    "for fr in results.forecasts:\n",
    "    if fr.samples is not None:\n",
    "        plot_probabilistic_forecast(\n",
    "            actual=fr.actual,\n",
    "            point_forecast=fr.point_forecast,\n",
    "            samples=fr.samples,\n",
    "            title=f\"{fr.model_name} — {fr.horizon}h forecast (window {fr.window_idx})\",\n",
    "        )\n",
    "        plt.show()\n",
    "        break\n",
    "else:\n",
    "    print(\"No probabilistic forecasts available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"../results/tables\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(out_dir / \"benchmark_results.csv\", index=False)\n",
    "print(f\"Results saved to {out_dir / 'benchmark_results.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
